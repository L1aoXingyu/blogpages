---
title: RoPE and Length Scaling
pubDatetime: 2023-08-03
description: Share my own
tags:
  - LLM
  - Positional Encoding
  - Math
  - pre-train
  - Length Extrapolation
  - Transformer
comments: true
draft: false
---

> Positional Encoding(PE) is very important for transformers, which is widely used in LLM nowadays. There are many different types of PE, and RoPE is one of them.
>
> RoPE is first proposed in RoFormer[^1], and applied in many popular transformer models, such as LLaMa[^2], open-sourced by Meta.
> This blog we will introduce the basic concept of RoPE, its derivation, and length extrapolation related it.

## Table of contents

## Background

Before we start, let's review some basic concepts of transformer and positional encoding.

### Transformer 101

Given a sequence $S_N$ with length $N$, $t_i$ is the $i$-th token in the sequence, and $x_i$ is the d-dim embedding of $t_i$. We can formulate $S_N$ and $E_N$ as:

$$
S_N = \{t_1, t_2, \dots, t_N\} \\
E_N = \{x_1, x_2, \dots, x_N\}
$$

Before computing self-attention, we need transform $x_i \in E_N$ to $Q_i$, $K_i$, $V_i$ with linear projection but adding extra positional information. We can formulate as:

$$
Q_i = f_q(x_i, p_i) \\
K_j = f_k(x_j, p_j) \\
V_j = f_v(x_j, p_j)
$$

where $p_i, p_j$ is the positional information of $x_i, x_j$ respectively, and $f_q$, $f_k$, $f_v$ are transform functions.

Then we can compute the self-attention scores as:

$$
\text{Attention}(Q_i, K_j) = \frac{exp(\frac{Q_i K_j^T}{\sqrt{d_k}})}{\sum_{j=1}^{N} exp(\frac{Q_i K_j^T}{\sqrt{d_k}})} \\
\text{Output}(Q_i) = \sum_{j=1}^{N} \text{Attention}(Q_i, K_j) V_j
$$

### Positional Encoding in Transformers

Transformers are parallel architectures which means they cannot capture the order of tokens in sequence. Positional encoding comes to rescue. Generally, there are two major approaches:

1. Fuse positional information into input embedding, which called **absolute positional embedding**;
2. Fuse positional information into self-attention scores, which called **relative positional embedding**.

**Absolute Positional Embedding(APE)**. The most common way is proposed in original transformer paper[^3], which is adding a fixed positional embedding to input embedding. Periodic function like sine and cosine functions are used to generate positional embedding. The formula is:

$$
PE_{(pos, 2i)} = sin(pos / 10000^{\frac{2i}{d}}) \\
PE_{(pos, 2i+1)} = cos(pos / 10000^{\frac{2i}{d}})
$$

where $pos$ is the position of token, $d$ is the dimension of embedding, and $i$ is for computing the index of dimension.

python code as follows:

```py
# pos: position of token
# seq_len: length of sequence
# d: dimension of embedding

def get_pos_embedding(pos, seq_len, d):
    pos_embedding = np.zeros((seq_len, d))
    for i in range(d):
        if i % 2 == 0:
            # even index using sine
            pos_embedding[:, i] = np.sin(pos / 10000 ** (i / d))
        else:
            # odd index using cosine
            pos_embedding[:, i] = np.cos(pos / 10000 ** ((i - 1) / d))
    return pos_embedding
```

It's evident that the characteristic of the sine/cosine positional encoding is periodical, hence it can be expected to have a certain degree of extrapolation. [^4]

Another common choice is to use learned version of APE, which is a trainable parameter, such as in GPT-3[^5].

**Relative Positional Embedding(RPE)**. Relative position encoding doesn't model the position information of each token.
Instead, it model the relative position when computing self-attention scores.

For example, _T5's Relative bias_[^6] first maps the relative distance $(i-j)$ between tokens at position i and j to a scalar bias value $b = f(i-j)$.
Then it is added to the dot product of query and key in the self-attention mechanism.

## Rotary Positional Embedding(RoPE)

Considering that APE is straightforward and easy to implement, and RPE is more intuitive and effective, RoPE can combine the advantages of both.

### Formulation

Given $q, k$, we can add absolute positional information as following:

$$
\widehat{q}_m = f(q, m) \\
\widehat{k}_n = f(k, n)
$$

$f(\cdot, m)$ is the function to add positional information to inputs. The equation below needs to be satisfied as attention computed by dot-product:

$$
<f(q, m), f(k, n)> = g(q, k, m-n)
$$

where $g(\cdot, \cdot, \cdot)$ is the function to compute self-attention scores. We can assume $f(q, 0) = q$ and $f(k, 0) = k$ safely.

Considering the 2-d situation and complex field, $q = (q_1, q_2) = q_1 + i * q_2$, we can get:

$$
\begin{align}
<q, k> &= q_1 * k_1 + q_2 * k_2 \\
q*\bar{k} &= (q_1 + i * q_2) * (k_1 - i * k_2) \\ &=  q_1 * k_1 + q_2 * k_2 + i * (q_1 * k_2 + q_2 * k_1) \\
<q, k> &= Re[q*\bar{k}]
\end{align}
$$

where $\bar{k} = k_1 - i * k_2$ is the conjugate of $k$ and $Re[\cdot]$ is the real part of complex number.

Then we can get:

$$
<f(q, m), f(k, n)> = Re[f(q, m) * \bar{f}(k, n)] = g(q, k, m-n)
$$

We can assume $f(q, m) * \bar{f}(k, n) = g(q, k, m-n)$ simply.

Using exponential form to represent complex number, we can get:

$$
\begin{align}
f(q, m) &= Re[f(q, m)] * e^{i * \theta_{f(q, m)}} \\
f(k, n) &= Re[f(k, n)] * e^{i * \theta_{f(k, n)}} \\
f(q, m) * \bar{f}(k, n) &= Re[f(q, m)] * Re[f(k, n)] * e^{i * (\theta_{f(q, m)} - \theta_{f(k, n)})} \\
&= Re[g(q, k, m-n)] * e^{i * \theta_{g(q, k, m-n)}}
\end{align}
$$

We can set these two equations equal according to Eq.(7) and Eq.(8):

$$
\begin{align}
Re[g(q, k, m-n)] &= Re[f(q, m)] * Re[f(k, n)] \\
\theta_{g(q, k, m-n)} &= \theta_{f(q, m)} - \theta_{f(k, n)}
\end{align}
$$

To solve Eq.(9) and Eq.(10), we can set $m=n=0$, then we can get:

$$
\begin{align}
Re[g(q, k, 0)] &= Re[f(q, m)] * Re[f(k, m)] \\
&= Re[f(q, 0)] * Re[f(k, 0)] \\
&= Re[q] * Re[k] \\
&= ||q|| * ||k||
\end{align}
$$

### General Form

## Length Extrapolation

### Linear Scaling

### NTK-aware Scaling

### Dynamic NTK Scaling

## Reference

[^1]: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864.pdf)
[^2]: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
[^3]: [Attention is all you need](https://arxiv.org/abs/1706.03762)
[^4]: [让研究人员绞尽脑汁的 Transformer 位置编码](https://kexue.fm/archives/8130)
[^5]: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
[^6]: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
