---
title: RoPE simple summary
pubDatetime: 2023-08-03
description: Share my own
tags:
  - LLM
  - Positional Encoding
  - Math
  - pre-train
  - Length Extrapolation
  - Transformer
comments: true
draft: true
---

> Positional Encoding(PE) is very important for transformers, which is widely used in LLM nowadays. There are many different types of PE, and RoPE is one of them.
>
> RoPE is first proposed in RoFormer[^1], and applied in many popular transformer models, such as LLaMa[^2], open-sourced by Meta.
> This blog we will introduce the basic concept of RoPE, its derivation, and length extrapolation related it.

## Table of contents

## Background

Before we start, let's review some basic concepts of transformer and positional encoding.

### Transformer 101

Given a sequence $S_N$ with length $N$, $t_i$ is the $i$-th token in the sequence, and $x_i$ is the d-dim embedding of $t_i$. We can formulate $S_N$ and $E_N$ as:

$$
S_N = \{t_1, t_2, \dots, t_N\} \\
E_N = \{x_1, x_2, \dots, x_N\}
$$

Before computing self-attention, we need transform $x_i \in E_N$ to $Q_i$, $K_i$, $V_i$ with linear projection but adding extra positional information. We can formulate as:

$$
Q_i = f_q(x_i, p_i)  \\
K_j = f_k(x_j, p_j)  \\
V_j = f_v(x_j, p_j)
$$

where $p_i, p_j$ is the positional information of $x_i, x_j$ respectively, and $f_q$, $f_k$, $f_v$ are transform functions.

Then we can compute the self-attention scores as:

$$
\text{Attention}(Q_i, K_j) = \frac{exp(\frac{Q_i K_j^T}{\sqrt{d_k}})}{\sum_{j=1}^{N} exp(\frac{Q_i K_j^T}{\sqrt{d_k}})} \\
\text{Output}(Q_i) = \sum_{j=1}^{N} \text{Attention}(Q_i, K_j) V_j
$$

### Positional Encoding in Transformers

Transformers are parallel architectures which means they cannot capture the order of tokens in sequence. Positional encoding comes to rescue. Generally, there are two major approaches:

1. Fuse positional information into input embedding, which called **absolute positional embedding**;
2. Fuse positional information into self-attention scores, which called **relative positional embedding**.

**Absolute Positional Embedding(APE)**. The most common way is proposed in original transformer paper[^3], which is adding a fixed positional embedding to input embedding. Periodic function like sine and cosine functions are used to generate positional embedding. The formula is:

$$
PE_{(pos, 2i)} = sin(pos / 10000^{\frac{2i}{d}}) \\
PE_{(pos, 2i+1)} = cos(pos / 10000^{\frac{2i}{d}})
$$

where $pos$ is the position of token, $d$ is the dimension of embedding, and $i$ is for computing the index of dimension.

python code as follows:

```py
# pos: position of token
# seq_len: length of sequence
# d: dimension of embedding

def get_pos_embedding(pos, seq_len, d):
    pos_embedding = np.zeros((seq_len, d))
    for i in range(d):
        if i % 2 == 0:
            # even index using sine
            pos_embedding[:, i] = np.sin(pos / 10000 ** (i / d))
        else:
            # odd index using cosine
            pos_embedding[:, i] = np.cos(pos / 10000 ** ((i - 1) / d))
    return pos_embedding
```

It's evident that the characteristic of the sine/cosine positional encoding is periodical, hence it can be expected to have a certain degree of extrapolation. [^4]

Another common choice is to use learned positional embedding, which is a trainable parameter, such as in GPT-3[^5].

**Relative Positional Embedding**.

## Rotary Positional Embedding(RoPE)

## Length Extrapolation

## Reference

[^1]: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864.pdf)
[^2]: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
[^3]: [Attention is all you need](https://arxiv.org/abs/1706.03762)
[^4]:
